{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "LLMSpanAttributes",
  "type": "object",
  "additionalProperties": false,
  "properties": {
    "langtrace.span.name": {
      "type": "string",
      "description": "Name of the span"
    },
    "langtrace.service.name": {
      "type": "string",
      "description": "Name of the service. Includes all supported service providers by langtrace"
    },
    "langtrace.service.type": {
      "type": "string",
      "description": "Type of the service. Allowed values: [llm, vectordb, framework]"
    },
    "langtrace.service.version": {
      "type": "string",
      "description": "Version of the service provider client"
    },
    "langtrace.version": {
      "type": "string"
    },
    "langtrace.sdk.name": {
      "type": "string"
    },
    "url.full": {
      "type": "string",
      "description": "Full URL of the request"
    },
    "url.path": {
      "type": "string",
      "description": "Path of the request"
    },
    "gen_ai.operation.name": {
      "type": "string",
      "description": "The name of the operation being performed."
    },
    "gen_ai.request.model": {
      "type": "string",
      "description": "Model name from the input request"
    },
    "gen_ai.response.model": {
      "type": "string",
      "description": "Model name from the response"
    },
    "gen_ai.request.temperature": {
      "type": "number",
      "format": "float",
      "description": "Temperature value from the input request"
    },
    "gen_ai.request.logit_bias": {
      "type": "string",
      "description": "Likelihood bias of the specified tokens the input request."
    },
    "gen_ai.request.logprobs": {
      "type": "boolean",
      "description": "Logprobs flag returns log probabilities."
    },
    "gen_ai.request.top_logprobs": {
      "type": "number",
      "format": "float",
      "description": "Integer between 0 and 5 specifying the number of most likely tokens to return."
    },
    "gen_ai.request.top_p": {
      "type": "number",
      "format": "float",
      "description": "Top P value from the input request"
    },
    "gen_ai.request.top_k": {
      "type": "number",
      "format": "float",
      "description": "Top K results to return from the input request"
    },
    "gen_ai.user": {
      "type": "string",
      "description": "User ID from the input request"
    },
    "gen_ai.prompt": {
      "type": "string",
      "description": "Prompt text from the input request"
    },
    "gen_ai.completion": {
      "type": "string",
      "description": "Completion text from the response. This will be an array of json objects with the following format {\"role\": \"\", \"content\": \"\"}. Role can be one of the following values: [system, user, assistant, tool]"
    },
    "gen_ai.request.stream": {
      "type": "boolean",
      "description": "Stream flag from the input request"
    },
    "gen_ai.request.encoding_formats": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Encoding formats from the input request. Allowed values: ['float', 'int8','uint8', 'binary', 'ubinary', 'base64']"
    },
    "gen_ai.completion.chunk": {
      "type": "string",
      "description": "Chunk text from the response"
    },
    "gen_ai.request.dimensions": {
      "type": "number",
      "format": "int32",
      "description": "Dimensions from the input request"
    },
    "gen_ai.response_id": {
      "type": "string",
      "description": "Response ID from the output response"
    },
    "gen_ai.response.finish_reasons": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "Array of reasons the model stopped generating tokens, corresponding to each generation received"
    },
    "gen_ai.system_fingerprint": {
      "type": "string",
      "description": "System fingerprint of the system that generated the response"
    },
    "gen_ai.request.documents": {
      "type": "string",
      "description": "Array of documents from the input request json stringified"
    },
    "gen_ai.request.is_search_required": {
      "type": "boolean",
      "description": "Search flag from the input request"
    },
    "gen_ai.request.tool_choice": {
      "type": "string",
      "description": "Tool choice from the input request"
    },
    "gen_ai.response.tool_calls": {
      "type": "string",
      "description": "Array of tool calls from the response json stringified"
    },
    "gen_ai.request.max_tokens": {
      "type": "number",
      "format": "int32",
      "description": "The maximum number of tokens the LLM generates for a request."
    },
    "gen_ai.usage.input_tokens": {
      "type": "number",
      "format": "int32",
      "description": "The number of tokens used in the llm prompt."
    },
    "gen_ai.usage.total_tokens": {
      "type": "number",
      "format": "int32",
      "description": "The total number of tokens used in the llm request."
    },
    "gen_ai.usage.output_tokens": {
      "type": "number",
      "format": "int32",
      "description": "The number of tokens in the llm response."
    },
    "gen_ai.usage.search_units": {
      "type": "number",
      "format": "int32",
      "description": "The number of search units used in the request."
    },
    "gen_ai.request.seed": {
      "type": "string",
      "description": "Seed from the input request"
    },
    "gen_ai.request.frequency_penalty": {
      "type": "number",
      "format": "float",
      "description": "Frequency penalty from the input request"
    },
    "gen_ai.request.presence_penalty": {
      "type": "number",
      "format": "float",
      "description": "Presence penalty from the input request"
    },
    "gen_ai.request.connectors": {
      "type": "string",
      "description": "An array of connectors from the input request json stringified"
    },
    "gen_ai.request.tools": {
      "type": "string",
      "description": "An array of tools from the input request json stringified"
    },
    "gen_ai.request.tool_results": {
      "type": "string",
      "description": "An array of tool results from the input request json stringified"
    },
    "gen_ai.request.embedding_inputs": {
      "type": "string",
      "description": "An array of embedding inputs from the input request json stringified"
    },
    "gen_ai.request.embedding_dataset_id": {
      "type": "string",
      "description": "Embedding dataset ID from the input request"
    },
    "gen_ai.request.embedding_input_type": {
      "type": "string",
      "description": "Embedding input type from the input request. Allowed values: [ 'search_document', 'search_query', 'classification', 'clustering']"
    },
    "gen_ai.request.embedding_job_name": {
      "type": "string",
      "description": "Embedding job name from the input request"
    },
    "gen_ai.image.size": {
      "type": "string",
      "description": "Image size from the input request. Allowed values: ['256x256', '512x512', '1024x1024']"
    },
    "gen_ai.request.response_format": {
      "type": "string",
      "description": "Response format from the input request. Allowed values: ['url', 'b64_json']"
    },
    "http.max.retries": {
      "type": "integer",
      "format": "int32"
    },
    "http.timeout": {
      "type": "integer",
      "format": "int64"
    },
    "gen_ai.cohere.rerank.query": {
      "type": "string",
      "description": "Query from the input request for the rerank api"
    },
    "gen_ai.cohere.rerank.results": {
      "type": "string",
      "description": "Results from the rerank api"
    }
  },
  "required": [
    "langtrace.service.name",
    "langtrace.service.type",
    "langtrace.sdk.name",
    "langtrace.version",
    "gen_ai.operation.name",
    "gen_ai.system",
    "gen_ai.request.model",
    "url.full",
    "url.path"
  ]
}